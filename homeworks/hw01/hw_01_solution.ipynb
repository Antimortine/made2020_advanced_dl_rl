{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть первая\n",
    "## Пункт 1\n",
    ">Рассмотрим очень простую стратегию: говорить stand, если у нас на руках комбинация в 19, 20 или 21 очко, во всех остальных случаях говорить hit. Используйте методы Монте-Карло, чтобы оценить выигрыш от этой стратегии.  \n",
    "\n",
    "Это эпизодическая задача, причём средняя длина эпизода небольшая (единицы ходов), поэтому положим $\\gamma = 1.$ По этой причине return ($G_t$) на каждом ходу будет просто равен reward на последнем ходу.  \n",
    "Самый простой способ оценить средний выигрыш стратегии - сгенерировать большое число эпизодов и усреднить выигрыш. Так и сделаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0', natural=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineStrategy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        player_sum, dealer_card, usable_ace = state\n",
    "        if player_sum < 19:\n",
    "            return 1  # hit\n",
    "        return 0  # stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = BaselineStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, strategy):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = strategy.get_action(state)\n",
    "        state, reward, done, _ =  env.step(action)\n",
    "        if done:\n",
    "            return reward\n",
    "\n",
    "def eval_strategy(env, strategy, num_iterations):\n",
    "    return np.mean([run_episode(env, strategy) for _ in trange(num_iterations)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 5000000/5000000 [04:33<00:00, 18261.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.1806987"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_strategy(env, baseline, 5000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но ничто нам не мешает немного перемудрить и построить Monte-Carlo estimation функции $V_{\\pi}(s)$, а затем взять её матожидание по распределению начальных состояний (которое тоже можно оценить методом Монте-Карло).  \n",
    "Для данной стратегии имеет значение лишь общая сумма на руках, но с суммой 21 нужно поступить хитрее: в начальном состоянии 21 можно получить только в виде пары (Ace, 10), что есть Natural blackjack и даёт reward=1.5 (если дилер тоже не наберёт 21, тогда reward=0), так что состоянием будем считать только сумму очков на руках, но 21 разобьём на 2 состояния (Natural blackjack или нет).  \n",
    "Оценим вероятности начальных состояний:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 5000000/5000000 [02:52<00:00, 29035.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "starts_counter = Counter(env.reset()[0] for _ in trange(5000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4     0.005900\n",
       "5     0.011858\n",
       "6     0.017857\n",
       "7     0.023689\n",
       "8     0.029617\n",
       "9     0.035516\n",
       "10    0.041429\n",
       "11    0.047430\n",
       "12    0.094695\n",
       "13    0.094860\n",
       "14    0.088761\n",
       "15    0.082924\n",
       "16    0.076803\n",
       "17    0.070999\n",
       "18    0.065013\n",
       "19    0.058906\n",
       "20    0.106309\n",
       "21    0.047434\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts_frequencies = pd.Series(starts_counter).sort_index()\n",
    "starts_frequencies /= starts_frequencies.sum()\n",
    "starts_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMeans:\n",
    "    def __init__(self, shape):\n",
    "        self.means = np.zeros(shape)\n",
    "        self.sizes = np.zeros(shape)\n",
    "    \n",
    "    def update(self, index, value):\n",
    "        m = self.means[index]\n",
    "        n = self.sizes[index]\n",
    "        self.means[index] = (m * n + value) / (n + 1)\n",
    "        self.sizes[index] += 1\n",
    "\n",
    "def get_states_and_reward(env, strategy):\n",
    "    state = env.reset()\n",
    "    states = []\n",
    "    while True:\n",
    "        states.append(state)\n",
    "        action = strategy.get_action(state)\n",
    "        state, reward, done, _ =  env.step(action)\n",
    "        if done:\n",
    "            return states, reward\n",
    "\n",
    "def estimate_V_for_baseline(env, strategy, num_iterations):\n",
    "    V = RunningMeans(env.observation_space[0].n)\n",
    "    for _ in trange(num_iterations):\n",
    "        #  Поскольку gamma==1, можно брать reward с последнего шага в качестве return на всех шагах,\n",
    "        #  а также обходить эпизод от начала к концу, а не наоборот\n",
    "        states, reward = get_states_and_reward(env, strategy)\n",
    "        for player_sum, dealer_card, usable_ace in states:\n",
    "            if player_sum == 21:\n",
    "                if len(states) == 1:  # Natural blackjack; будем его записывать в ячейку 21\n",
    "                    V.update(21, reward)\n",
    "                else:  # 21 получено другим способом; сохраним отдельно в ячейку 22 - такой суммы всё равно не может быть\n",
    "                    V.update(22, reward)\n",
    "            else:\n",
    "                V.update(player_sum, reward)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 5000000/5000000 [05:28<00:00, 15204.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.34953906, -0.36656564, -0.38691354, -0.41579624, -0.41458222,\n",
       "       -0.16153505, -0.02335208,  0.04528941, -0.41877361, -0.44277624,\n",
       "       -0.47682696, -0.51144324, -0.54092291, -0.5692912 , -0.59529848,\n",
       "        0.26731447,  0.58007819,  1.31888521,  0.87947545])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Получить меньше 4 и больше 21 очков невозможно\n",
    "#  В ячейке 21 хранится оценка для состояния Natural blackjack\n",
    "#  В ячейке 22 хранится оценка для суммы 21, полученной иным способом\n",
    "baseline_V = estimate_V_for_baseline(env, baseline, 5000000).means[4:23]\n",
    "baseline_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для Natural blackjack оценка не равна 1.5, потому что дилер тоже может набрать 21 и будет ничья"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1870863467778301"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(starts_frequencies.values * baseline_V[:-1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
